{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBtO6lLyw7d+ni1lNJnI5G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gattuvamsi/Deep_Learning/blob/main/2203A52127_DL_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function f(x)\n",
        "import random\n",
        "def f(x):\n",
        "    return 5 * x**4 + 3 * x**2 + 10\n",
        "\n",
        "# Define the derivative of f(x)\n",
        "def df(x):\n",
        "    return 20*(x**3) + 6*x\n",
        "\n",
        "# Gradient Descent Algorithm\n",
        "def gradient_descent(learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
        "    # Initialize x randomly or at a starting point\n",
        "    x = random.uniform(-10,10)  # Starting point (can be any value)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # Compute the gradient (derivative) at the current x\n",
        "        gradient = df(x)\n",
        "\n",
        "        # Update x using the gradient descent rule\n",
        "        x_new = x - learning_rate * gradient\n",
        "\n",
        "        # Check for convergence (if the change in x is very small)\n",
        "        if abs(x_new - x) < tolerance:\n",
        "            print(f\"Converged after {i} iterations.\")\n",
        "            break\n",
        "\n",
        "        # Update x\n",
        "        x = x_new\n",
        "\n",
        "    return x\n",
        "\n",
        "# Run gradient descent\n",
        "optimal_x = gradient_descent()\n",
        "print(f\"The value of x at which f(x) is minimized: {optimal_x}\")\n",
        "print(f\"Minimum value of f(x): {f(optimal_x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QLHd8JGPhv-",
        "outputId": "1cc4505a-7bd8-42c5-c8f7-9334930612c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 165 iterations.\n",
            "The value of x at which f(x) is minimized: 1.6380719253121603e-05\n",
            "Minimum value of f(x): 10.000000000804985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function g(x, y)\n",
        "import random\n",
        "def g(x, y):\n",
        "    return 3 * x**2 + 5 * (2.71828 ** (-y)) + 10  # 2.71828 is an approximation of e\n",
        "\n",
        "# Define the partial derivatives of g(x, y)\n",
        "def dg_dx(x):\n",
        "    return 6 * x  # Partial derivative with respect to x\n",
        "\n",
        "def dg_dy(y):\n",
        "    return -5 * (2.71828 ** (-y))  # Partial derivative with respect to y\n",
        "\n",
        "# Gradient Descent Algorithm\n",
        "def gradient_descent(learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
        "    # Initialize x and y randomly or at starting points\n",
        "    x = random.uniform(-10,10)  # Starting point for x\n",
        "    y = random.uniform(-10,10) # Starting point for y\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # Compute the gradients (partial derivatives) at the current x and y\n",
        "        grad_x = dg_dx(x)\n",
        "        grad_y = dg_dy(y)\n",
        "\n",
        "        # Update x and y using the gradient descent rule\n",
        "        x_new = x - learning_rate * grad_x\n",
        "        y_new = y - learning_rate * grad_y\n",
        "\n",
        "        # Check for convergence (if the changes in x and y are very small)\n",
        "        if abs(x_new - x) < tolerance and abs(y_new - y) < tolerance:\n",
        "            print(f\"Converged after {i} iterations.\")\n",
        "            break\n",
        "\n",
        "        # Update x and y\n",
        "        x = x_new\n",
        "        y = y_new\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# Run gradient descent\n",
        "optimal_x, optimal_y = gradient_descent()\n",
        "print(f\"The value of x at which g(x, y) is minimized: {optimal_x}\")\n",
        "print(f\"The value of y at which g(x, y) is minimized: {optimal_y}\")\n",
        "print(f\"Minimum value of g(x, y): {g(optimal_x, optimal_y)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CJx0UyOTSJc",
        "outputId": "23d56b89-3e36-4865-bbcc-eff742dd360b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x at which g(x, y) is minimized: 4.923759493938605e-27\n",
            "The value of y at which g(x, y) is minimized: 9.175100653994368\n",
            "Minimum value of g(x, y): 10.000517937194132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sigmoid function z(x)\n",
        "import random\n",
        "def z(x):\n",
        "    return 1 / (1 + 2.71828 ** (-x))  # 2.71828 is an approximation of e\n",
        "\n",
        "# Define the derivative of the sigmoid function z(x)\n",
        "def dz_dx(x):\n",
        "    return z(x) * (1 - z(x))  # Derivative of sigmoid function\n",
        "\n",
        "# Gradient Descent Algorithm\n",
        "def gradient_descent(learning_rate=0.1, max_iterations=1000, tolerance=1e-6):\n",
        "    # Initialize x randomly or at a starting point\n",
        "    x = random.uniform(-10,10)# Starting point (can be any value)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # Compute the gradient (derivative) at the current x\n",
        "        gradient = dz_dx(x)\n",
        "\n",
        "        # Update x using the gradient descent rule\n",
        "        x_new = x - learning_rate * gradient\n",
        "\n",
        "        # Check for convergence (if the change in x is very small)\n",
        "        if abs(x_new - x) < tolerance:\n",
        "            print(f\"Converged after {i} iterations.\")\n",
        "            break\n",
        "\n",
        "        # Update x\n",
        "        x = x_new\n",
        "\n",
        "    return x\n",
        "\n",
        "# Run gradient descent\n",
        "optimal_x = gradient_descent()\n",
        "print(f\"The value of x at which z(x) is minimized: {optimal_x}\")\n",
        "print(f\"Minimum value of z(x): {z(optimal_x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY4pQaOuUfSD",
        "outputId": "7864f55d-c2db-41ed-de50-6f5ef72537ea"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x at which z(x) is minimized: -4.667697298341416\n",
            "Minimum value of z(x): 0.009306481224467096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_M_C(learning_rate=0.01, epochs=1000):\n",
        "    M=random.uniform(-10,10)\n",
        "    C=random.uniform(-10,10)  # Initial values\n",
        "    X = 2  # Given input\n",
        "    ExpectedOutput = 0.5  # Expected output\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        PredictedOutput = M * X + C\n",
        "        error = ExpectedOutput - PredictedOutput\n",
        "        grad_M = -2 * X * error  # Derivative w.r.t M\n",
        "        grad_C = -2 * error  # Derivative w.r.t C\n",
        "\n",
        "        M -= learning_rate * grad_M\n",
        "        C -= learning_rate * grad_C\n",
        "\n",
        "    return M, C\n",
        "\n",
        "# Run the function and print results\n",
        "optimal_M, optimal_C = gradient_descent_M_C()\n",
        "print(\"Optimal M:\", optimal_M)\n",
        "print(\"Optimal C:\", optimal_C)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKBxiXxUW1Fq",
        "outputId": "7c3224b7-4965-4975-b25e-fc2499f0b252"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal M: 3.7566302456734304\n",
            "Optimal C: -7.0132604913468555\n"
          ]
        }
      ]
    }
  ]
}